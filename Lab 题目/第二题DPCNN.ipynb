{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import torch.utils.data as Data\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(label):\n",
    "    for i in range(len(label)):\n",
    "        label[i] = int(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_split(str_list,cut_for_search=False,cut_all=False):\n",
    "    def to_str(review):\n",
    "        for i in range(len(review)):\n",
    "            review[i] = str(review[i])\n",
    "    to_str(str_list)\n",
    "    for i in range(len(str_list)):\n",
    "        if cut_for_search:\n",
    "            str_split=list(jieba.cut_for_search(str_list[i]))\n",
    "        else:\n",
    "            str_split=list(jieba.cut(str_list[i],cut_all=cut_all))\n",
    "        str_split = pd.Series(str_split)[pd.Series(str_split).apply(len)>0]\n",
    "        punctuation=[':','“','”',',','…','。','.','~','、','～','......','，',\n",
    "                     '♪','(',')','o','。。。','……','•','。。。。。。']\n",
    "        str_split = str_split[~str_split.isin(punctuation)]\n",
    "        str_list[i]=\" \".join(str_split)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=\"/root/autodl-tmp/data/评论情感分析数据集/waimai_10k.csv\"\n",
    "file2=\"/root/autodl-tmp/data/评论情感分析数据集/ChnSentiCorp_htl_all.csv\"\n",
    "data1=pd.read_csv(file1)\n",
    "data2=pd.read_csv(file2)\n",
    "f1_review_pos,f1_label_pos=list(data1[\"review_pos\"])[:4000],list(data1[\"label_pos\"])[:4000]\n",
    "f1_review_neg,f1_label_neg=list(data1[\"review_neg\"])[:7987],list(data1[\"label_neg\"])[:7987]\n",
    "f2_review_pos,f2_label_pos=list(data2[\"review_pos\"])[:5322],list(data2[\"label_pos\"])[:5322]\n",
    "f2_review_neg,f2_label_neg=list(data2[\"review_neg\"])[:2444],list(data2[\"label_neg\"])[:2444]\n",
    "to_split(f1_review_pos)\n",
    "to_split(f1_review_neg)\n",
    "to_split(f2_review_pos)\n",
    "to_split(f2_review_neg)\n",
    "to_int(f1_label_pos)\n",
    "to_int(f1_label_neg)\n",
    "to_int(f2_label_pos)\n",
    "to_int(f2_label_neg)\n",
    "train_data=(f1_review_pos[:-500]+f1_review_neg[:-500]+f2_review_pos[:-500]+f2_review_neg[:-500]\n",
    "            ,f1_label_pos[:-500]+f1_label_neg[:-500]+f2_label_pos[:-500]+f2_label_neg[:-500])\n",
    "test_data=(f1_review_pos[-500:]+f1_review_neg[-500:]+f2_review_pos[-500:]+f2_review_neg[-500:]\n",
    "            ,f1_label_pos[-500:]+f1_label_neg[-500:]+f2_label_pos[-500:]+f2_label_neg[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, num_steps=100):\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=2)\n",
    "    train_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),\n",
    "                                batch_size,\n",
    "                                is_train=True)\n",
    "    test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),\n",
    "                               batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, channel_size):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "\n",
    "        self.channel_size = channel_size\n",
    "        self.maxpool = nn.Sequential(\n",
    "            nn.ConstantPad1d(padding=(0, 1), value=0),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size, kernel_size=3, padding=1),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shortcut = self.maxpool(x)\n",
    "        x = self.conv(x_shortcut)\n",
    "        x = x + x_shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_features, word_embedding_dimension, max_sentence_length, num_classes):\n",
    "        super(DPCNN, self).__init__()\n",
    "        self.max_features = max_features \n",
    "        self.embed_size = word_embedding_dimension\n",
    "        self.maxlen = max_sentence_length \n",
    "        self.num_classes = num_classes \n",
    "        self.channel_size = 250\n",
    "        self.embedding = nn.Embedding(self.max_features, self.embed_size)\n",
    "        torch.nn.init.normal_(self.embedding.weight.data, mean=0, std=0.01)\n",
    "        self.region_embedding = nn.Sequential(\n",
    "            nn.Conv1d(self.embed_size, self.channel_size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_features=self.channel_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(num_features=self.channel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(self.channel_size, self.channel_size, kernel_size=3, padding=1), \n",
    "        )\n",
    "        self.seq_len = self.maxlen\n",
    "        resnet_block_list = []\n",
    "        while (self.seq_len > 2):\n",
    "            resnet_block_list.append(ResnetBlock(self.channel_size))\n",
    "            self.seq_len = self.seq_len // 2  \n",
    "        self.resnet_layer = nn.Sequential(*resnet_block_list)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.channel_size * self.seq_len, self.num_classes),\n",
    "            nn.BatchNorm1d(self.num_classes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.num_classes, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, description_word_seq):\n",
    "        x = self.embedding(description_word_seq)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.region_embedding(x)\n",
    "        x = self.conv_block(x)\n",
    "        x = self.resnet_layer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "    def predict(self,vocab,sentence,cut_all=False):\n",
    "       to_split(sentence,cut_all)\n",
    "       sentence=sentence[0].split()\n",
    "       if len(sentence)>self.maxlen:\n",
    "           sentence=sentence[:self.maxlen]\n",
    "       else:\n",
    "           sentence=sentence+['<pad>']*(self.maxlen-len(sentence))\n",
    "       sequence = torch.tensor(vocab[sentence])\n",
    "       label = torch.argmax(self.forward(sequence.view(1,-1).to(d2l.try_gpu())), dim=1)\n",
    "       return 'positive' if label.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir='/root/autodl-tmp/data/评论情感分析数据集'\n",
    "        with open(os.path.join(data_dir, embedding_name), 'r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, d2l.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[d2l.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dimension,max_sentence_length,num_classes=300,100,2\n",
    "train_iter,test_iter,vocab=load_data(128,max_sentence_length)\n",
    "net = DPCNN(len(vocab), word_embedding_dimension, max_sentence_length, num_classes)\n",
    "def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv1d:\n",
    "            nn.init.kaiming_uniform_(m.weight)\n",
    "net.apply(init_weights)\n",
    "glove_embedding = TokenEmbedding('sgns.weibo.word')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.data.copy_(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(net, X, y, loss, trainer, device,scheduler=False):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, num_epochs, loss, trainer, device,scheduler=False):\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    net = net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch(net, features, labels, loss, trainer, device,scheduler)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[3],None))\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "        \n",
    "                    \n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, train_iter, test_iter,lr, num_epochs, loss, device,\n",
    "                      scheduler=False,\n",
    "                      param_group=True):\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "                     if name not in [\"embedding.weight\"]]\n",
    "        trainer = torch.optim.Adam([{'params': net.embedding.parameters(),'lr': lr * 0.1},\n",
    "                                    {'params': params_1x}],\n",
    "                                   lr=lr,weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=0.001)\n",
    "        \n",
    "    if scheduler:\n",
    "        scheduler=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(trainer,T_0=4,T_mult=2)\n",
    "    train(net, train_iter, test_iter, num_epochs, loss, trainer, device,scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 1e-3, 40\n",
    "device=d2l.try_gpu()\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "train_fine_tuning(net, train_iter, test_iter,lr, num_epochs, loss, device,scheduler=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.predict(vocab=vocab,\n",
    "            sentence=[\"你送的真的好慢!\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
